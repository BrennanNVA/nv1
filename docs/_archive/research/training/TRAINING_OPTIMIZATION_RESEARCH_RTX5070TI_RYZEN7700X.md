# Comprehensive Training Optimization Research
## NVIDIA RTX 5070 Ti + AMD Ryzen 7700x System

**Research Date:** January 2025
**Hardware Configuration:**
- GPU: NVIDIA RTX 5070 Ti (Blackwell Architecture)
- CPU: AMD Ryzen 7 7700x
- System: Nova Aetus Trading System

---

## Executive Summary

This document consolidates extensive research (10+ minutes of deep web research) on optimizing XGBoost training for the RTX 5070 Ti and Ryzen 7700x configuration. Key findings include GPU memory optimization, CPU-GPU coordination, data pipeline efficiency, and hyperparameter tuning strategies.

---

## Hardware Specifications & Capabilities

### NVIDIA RTX 5070 Ti Specifications

**Core Specifications:**
- **CUDA Cores:** 8,960 (70 SMs)
- **VRAM:** 16GB GDDR7
- **Memory Bandwidth:** 896 GB/s (256-bit bus)
- **Architecture:** Blackwell (GB203 GPU)
- **PCIe:** 5.0 support (backward compatible with 4.0)
- **Tensor Cores:** 280 (4th gen)
- **RT Cores:** 70 (3rd gen)
- **TDP:** 300W
- **Compute Capability:** 12.0 (CUDA 12.0+ required)

**Performance Characteristics:**
- Approximately **40% faster** than RTX 4070 Ti for ResNet/EfficientNet training
- Optimal batch sizes: **64-128** for most configurations
- **16GB VRAM** enables training larger models without external memory
- GDDR7 provides **27% improvement** in memory bus throughput vs RTX 4070 Ti

### AMD Ryzen 7 7700x Specifications

**Core Specifications:**
- **Cores/Threads:** 8 cores / 16 threads
- **Base Clock:** 4.5 GHz
- **Boost Clock:** Up to 5.4 GHz
- **PCIe:** 4.0 (24 lanes from CPU)
- **Memory:** DDR5 support (optimal: DDR5-6000 CL30)
- **TDP:** 105W

**Bottleneck Analysis:**
- PCIe 4.0 provides **sufficient bandwidth** for RTX 5070 Ti (not a bottleneck)
- CPU-GPU data transfer: PCIe 4.0 x16 = ~64 GB/s (adequate for most workloads)
- **8 cores/16 threads** sufficient for data preprocessing while GPU trains
- DDR5-6000 recommended to avoid memory bottlenecks

---

## XGBoost GPU Training Optimization Strategies

### 1. Core GPU Configuration

**Essential Parameters:**
```python
{
    "tree_method": "hist",  # NOT "gpu_hist" in XGBoost 2.0+
    "device": "cuda",       # Automatically uses GPU
    "predictor": "gpu_predictor",  # For inference on GPU
    "gpu_id": 0,            # Use first GPU
    "max_bin": 512,          # Default, good for RTX 5070 Ti
}
```

**Key Findings:**
- XGBoost 2.0+ uses `tree_method="hist"` with `device="cuda"` (not `"gpu_hist"`)
- `max_bin=512` is optimal for 16GB VRAM (default works well)
- GPU training is **10-46x faster** than CPU for large datasets
- Sweet spot: datasets **2-24GB** benefit most from GPU acceleration

### 2. Memory Optimization Techniques

#### QuantileDMatrix for Large Datasets

**When to Use:**
- Datasets approaching or exceeding GPU memory
- Multiple symbols training concurrently
- 88+ technical indicators creating large feature sets

**Implementation:**
```python
import xgboost as xgb

# Instead of DMatrix, use QuantileDMatrix
dtrain = xgb.QuantileDMatrix(X_train, y_train)
# Automatically reduces GPU memory by 5x
```

**Benefits:**
- **5x memory reduction** vs standard DMatrix
- Pre-quantizes features, reducing VRAM usage
- Maintains training speed
- **Recommended for datasets >8GB** on 16GB VRAM GPU

#### External Memory (ExtMemQuantileDMatrix)

**When to Use:**
- Datasets >16GB (exceed GPU VRAM)
- Training on multiple symbols with full history
- Terabyte-scale datasets

**Requirements:**
- RMM (RAPIDS Memory Manager) with async memory resource
- Fast NVMe storage for cache (not slow disks)
- PCIe 4.0+ for host-to-device transfers

**Implementation:**
```python
import rmm
import xgboost as xgb

# Setup RMM pool
mr = rmm.mr.PoolMemoryResource(rmm.mr.CudaAsyncMemoryResource())
rmm.mr.set_current_device_resource(mr)

# Use external memory iterator
dtrain = xgb.ExtMemQuantileDMatrix(data_iterator, ...)
```

**Performance Notes:**
- Accept I/O overhead (slower than in-memory but enables >1TB datasets)
- Use `extmem_single_page=True` to reduce PCIe overhead
- Store cache on fast NVMe (OS caching helps)

### 3. RAPIDS Memory Manager (RMM) Integration

**Why Use RMM:**
- **Faster memory allocation** (pool allocator vs cudaMalloc)
- **Reduced fragmentation** (memory pool management)
- **Asynchronous allocation** (overlaps with computation)
- **Shared memory pool** with other RAPIDS libraries

**Setup:**
```python
import rmm

# Initialize RMM pool (recommended for external memory)
mr = rmm.mr.PoolMemoryResource(
    rmm.mr.CudaAsyncMemoryResource(),
    initial_pool_size=8 * 1024**3,  # 8GB initial pool
    maximum_pool_size=14 * 1024**3  # 14GB max (leave 2GB for system)
)
rmm.mr.set_current_device_resource(mr)
```

**Benefits:**
- **5-10x faster** memory allocation
- Reduces memory fragmentation
- Critical for external memory training
- Enables overlapping data transfer with computation

### 4. Gradient-Based Sampling

**New in XGBoost 3.0+:**
```python
{
    "subsample": 0.2,  # Lower sampling rate
    "sampling_method": "gradient_based",  # Key parameter
}
```

**Benefits:**
- Enables **lower subsample rates** (0.2) without accuracy loss
- Reduces memory usage significantly
- Works with external memory
- Maintains model quality

---

## CPU-GPU Coordination Optimization

### PCIe 4.0 Bandwidth Analysis

**Ryzen 7700x PCIe 4.0 Capabilities:**
- **24 PCIe 4.0 lanes** from CPU
- **x16 slot for GPU** = ~64 GB/s theoretical
- **x4 for M.2 NVMe** (data storage)
- **Remaining lanes** for chipset

**Bottleneck Assessment:**
- PCIe 4.0 x16 provides **sufficient bandwidth** for RTX 5070 Ti
- **Not a bottleneck** for XGBoost training (data transfer is minimal after initial load)
- GPU memory bandwidth (896 GB/s) is the limiting factor, not PCIe

**Optimization Strategies:**
1. **Pre-load data to GPU** before training starts
2. **Use QuantileDMatrix** to reduce transfer size
3. **Overlap computation with data transfer** (RMM helps)
4. **Batch data loading** to minimize PCIe overhead

### CPU Preprocessing While GPU Trains

**Optimal Workflow:**
```
1. CPU: Fetch next symbol's data (async)
2. CPU: Calculate technical indicators (Polars, parallel)
3. CPU: Generate NPMM labels
4. GPU: Train model (while CPU prepares next symbol)
5. Repeat
```

**Ryzen 7700x Utilization:**
- **8 cores / 16 threads** sufficient for:
  - Data fetching (async I/O)
  - Polars feature calculation (parallel)
  - Label generation
  - Database writes
- **No CPU bottleneck** expected with proper async design

**Implementation:**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

# Use async for I/O-bound operations
async def fetch_and_preprocess(symbol):
    # Fetch data
    data = await data_loader.fetch_async(symbol)
    # Calculate features (CPU-bound, use thread pool)
    features = await loop.run_in_executor(
        executor, calculate_features, data
    )
    return features

# Train on GPU while preparing next symbol
async def train_universe(symbols):
    for i, symbol in enumerate(symbols):
        if i > 0:
            # Train previous symbol on GPU
            await train_model_async(previous_features)
        # Prepare next symbol on CPU
        current_features = await fetch_and_preprocess(symbol)
```

---

## Data Pipeline Optimization

### Polars Integration with XGBoost

**Current Implementation:**
```python
# Convert Polars to NumPy (current approach)
X_train_np = X_train.to_numpy()
y_train_np = y_train.to_numpy()
```

**Optimization Opportunities:**

1. **Direct Polars to XGBoost (XGBoost 3.0+):**
   - XGBoost can accept Polars DataFrames directly
   - Avoids NumPy conversion overhead
   - Maintains memory efficiency

2. **Lazy Evaluation:**
   ```python
   # Use Polars lazy evaluation
   df = pl.scan_csv("data.csv")
   features = df.select(feature_cols).collect(streaming=True)
   # Streaming reduces memory usage
   ```

3. **Chunked Processing:**
   ```python
   # Process in chunks to reduce peak memory
   for batch in df.collect_batches(chunk_size=10000):
       process_batch(batch)
   ```

**Memory Efficiency:**
- Polars streaming: **2-7x faster** than in-memory for large datasets
- Reduces CPU RAM usage during feature calculation
- Enables processing datasets larger than RAM

### Feature Selection for 88+ Indicators

**Problem:**
- 88+ technical indicators create high-dimensional feature space
- GPU memory scales with number of features
- Feature selection critical to avoid overfitting

**Solutions:**

1. **XGBoost Feature Importance:**
   ```python
   # After initial training
   importance = model.get_booster().get_score(importance_type="gain")
   # Select top 20-30 features
   top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:30]
   ```

2. **Correlation Filtering:**
   - Remove redundant features (high correlation)
   - Use Polars for efficient correlation calculation

3. **Mutual Information:**
   - Select features with highest predictive power
   - Use scikit-learn's mutual_info_classif

4. **Recursive Feature Elimination:**
   - Start with all features
   - Iteratively remove least important
   - Stop when performance degrades

**Recommended Approach:**
- Start with **raw OHLCV** + **top 20-30 indicators**
- Add indicators selectively (only if they improve OOS performance)
- Use walk-forward validation to prevent overfitting

---

## Optuna Hyperparameter Optimization

### GPU-Accelerated Optuna Setup

**Key Configuration:**
```python
import optuna
from optuna.integration import XGBoostPruningCallback

def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 7),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "gamma": trial.suggest_float("gamma", 0.0, 0.5),
        "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 1.0, log=True),
        "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 2.0, log=True),
        # GPU parameters
        "tree_method": "hist",
        "device": "cuda",
        "predictor": "gpu_predictor",
    }

    # Pruning callback for early termination
    pruning_callback = XGBoostPruningCallback(trial, "validation_0-logloss")

    model = xgb.XGBClassifier(**params)
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        callbacks=[pruning_callback],
        verbose=False
    )

    return model.score(X_val, y_val)

# Study configuration
sampler = optuna.samplers.TPESampler(seed=42)
pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)

study = optuna.create_study(
    direction="maximize",
    sampler=sampler,
    pruner=pruner
)

study.optimize(objective, n_trials=100, timeout=3600)
```

### Optimization Strategies

**1. Parallel Trials:**
- GPU training is fast (10-46x speedup)
- Run **multiple trials in parallel** (CPU-bound Optuna overhead)
- Use `n_jobs` parameter or multiprocessing

**2. Pruning Configuration:**
- **MedianPruner** with `n_warmup_steps=10` (recommended)
- **XGBoostPruningCallback** for per-trial early stopping
- Saves time on unpromising trials

**3. Search Space:**
- **TPESampler** (Tree-structured Parzen Estimator) - best for XGBoost
- Log-scale for learning_rate, reg_alpha, reg_lambda
- Integer ranges for n_estimators, max_depth

**4. Trial Budget:**
- **100-200 trials** recommended for thorough search
- **1-hour timeout** prevents runaway optimization
- GPU speed enables more trials in same time

### Early Stopping Integration

**Best Practices:**
```python
{
    "early_stopping_rounds": 50,  # Patience for validation improvement
    "eval_set": [(X_val, y_val)],  # Required for early stopping
    "eval_metric": "logloss",      # Metric to monitor
}
```

**Validation Set Size:**
- **20% of training data** (standard)
- Time-based split (not random) for time series
- Ensure sufficient samples for reliable early stopping

**Benefits:**
- Prevents overfitting
- Finds optimal n_estimators automatically
- Reduces training time
- Works with Optuna pruning

---

## Training Workflow Optimization

### Walk-Forward Validation with GPU

**Current Implementation:**
- Walk-forward optimization with 2-year IS, 6-month OOS
- Step size: 3 months
- Expanding or rolling window

**GPU Optimization:**
- **Pre-compute all windows** (CPU: Polars)
- **Train sequentially on GPU** (faster than CPU)
- **Parallel validation** (CPU can validate while GPU trains next)

**Memory Management:**
- Clear GPU memory between windows
- Use `torch.cuda.empty_cache()` if using PyTorch
- XGBoost automatically releases memory after training

### Multiple Symbol Training

**Sequential vs Parallel:**

**Sequential (Current):**
```python
for symbol in symbols:
    model = train_symbol(symbol)  # GPU training
    save_model(model)
```

**Optimized Parallel:**
```python
# CPU prepares data while GPU trains
async def train_universe_optimized(symbols):
    tasks = []
    for symbol in symbols:
        # Prepare data on CPU (async)
        data_task = prepare_data_async(symbol)
        tasks.append(data_task)

    # Train on GPU as data becomes available
    for i, symbol in enumerate(symbols):
        features = await tasks[i]
        model = await train_on_gpu_async(features)
        save_model(model)
```

**Benefits:**
- Overlaps CPU preprocessing with GPU training
- Maximizes GPU utilization
- Reduces total training time

### Batch Size Optimization

**For RTX 5070 Ti (16GB VRAM):**

**Small Datasets (<1GB):**
- Load entire dataset to GPU
- No batching needed
- GPU overhead may make CPU faster

**Medium Datasets (1-8GB):**
- Use QuantileDMatrix
- Process in single batch
- Optimal for RTX 5070 Ti

**Large Datasets (8-16GB):**
- Use QuantileDMatrix with data iterator
- Batch size: **8GB per batch** (recommended)
- External memory if >16GB

**Very Large Datasets (>16GB):**
- ExtMemQuantileDMatrix
- RMM pool allocator
- Fast NVMe cache storage

---

## Performance Benchmarks & Expectations

### RTX 5070 Ti vs RTX 4070 Ti

**Training Speed:**
- **~40% faster** than RTX 4070 Ti for ResNet/EfficientNet
- Similar improvement expected for XGBoost
- **16GB vs 12GB VRAM** enables larger models

**Memory Bandwidth:**
- **896 GB/s** (RTX 5070 Ti) vs **504 GB/s** (RTX 4070 Ti)
- **78% improvement** in memory bandwidth
- Significant for large feature sets

### Expected Training Times

**Single Symbol (4 years daily data, 88 features):**
- **CPU (Ryzen 7700x):** ~10-15 minutes
- **GPU (RTX 5070 Ti):** ~30-60 seconds
- **Speedup: 10-30x**

**Universe Training (5 symbols):**
- **CPU:** ~50-75 minutes
- **GPU:** ~3-5 minutes
- **Speedup: 15-25x**

**With Optuna (100 trials):**
- **CPU:** ~16-25 hours
- **GPU:** ~1-2 hours
- **Speedup: 15-25x**

---

## Implementation Recommendations

### Immediate Optimizations

1. **Enable QuantileDMatrix:**
   ```python
   # In trainer.py, replace DMatrix with QuantileDMatrix
   dtrain = xgb.QuantileDMatrix(X_train, y_train)
   ```

2. **Add RMM Support:**
   ```python
   # At startup, initialize RMM
   import rmm
   mr = rmm.mr.PoolMemoryResource(rmm.mr.CudaAsyncMemoryResource())
   rmm.mr.set_current_device_resource(mr)
   ```

3. **Optimize Optuna:**
   - Use XGBoostPruningCallback
   - Increase n_trials to 100-200 (GPU enables more)
   - Use TPESampler with MedianPruner

4. **Feature Selection:**
   - Implement top-30 feature selection
   - Reduce from 88+ to 30-40 most important
   - Use walk-forward validation to validate

### Medium-Term Optimizations

1. **Async Training Pipeline:**
   - Overlap CPU preprocessing with GPU training
   - Use asyncio for I/O operations
   - Thread pool for CPU-bound feature calculation

2. **External Memory Support:**
   - Implement ExtMemQuantileDMatrix for large datasets
   - Setup RMM with async memory resource
   - Use fast NVMe for cache storage

3. **Gradient-Based Sampling:**
   - Test `sampling_method="gradient_based"` with `subsample=0.2`
   - Reduces memory usage significantly
   - Maintains model quality

### Long-Term Optimizations

1. **Multi-Symbol Parallel Training:**
   - Use Dask for distributed training (if multiple GPUs)
   - For single GPU: optimize sequential training with async

2. **Advanced Memory Management:**
   - Implement memory pooling
   - Pre-allocate GPU memory
   - Monitor and optimize memory usage

3. **Hardware Upgrades:**
   - Consider DDR5-6000 CL30 RAM (if not already)
   - Fast NVMe for external memory cache
   - PCIe 5.0 motherboard (future-proof, not required now)

---

## Troubleshooting Common Issues

### GPU Not Utilized

**Symptoms:**
- Training time similar to CPU
- `nvidia-smi` shows low GPU utilization

**Solutions:**
1. Verify CUDA installation: `nvidia-smi`
2. Check XGBoost GPU support: `xgboost.get_config()`
3. Ensure `device="cuda"` and `tree_method="hist"`
4. Verify CUDA 12.0+ and Compute Capability 12.0

### Out of Memory (OOM) Errors

**Symptoms:**
- CUDA OOM during training
- GPU memory exhausted

**Solutions:**
1. Use QuantileDMatrix (reduces memory 5x)
2. Reduce `max_bin` (512 â†’ 256)
3. Enable gradient-based sampling
4. Use external memory for very large datasets
5. Reduce feature count (feature selection)

### Slow First Iteration

**Symptoms:**
- First training iteration very slow
- Subsequent iterations faster

**Causes:**
- CUDA initialization overhead
- Memory allocation
- Data transfer to GPU

**Solutions:**
1. Pre-warm GPU with dummy training
2. Pre-allocate memory with RMM
3. Use QuantileDMatrix (faster initialization)

### PCIe Bottleneck

**Symptoms:**
- GPU underutilized
- High CPU usage during training

**Solutions:**
1. Pre-load data to GPU before training
2. Use QuantileDMatrix (smaller transfer size)
3. Verify PCIe 4.0 x16 connection
4. Check motherboard BIOS settings

---

## Research Sources & References

### Official Documentation
1. XGBoost GPU Support: https://xgboost.readthedocs.io/en/stable/gpu/
2. XGBoost External Memory: https://xgboost.readthedocs.io/en/stable/tutorials/external_memory.html
3. RAPIDS Memory Manager: https://github.com/rapidsai/rmm
4. Optuna XGBoost Integration: https://optuna.readthedocs.io/

### Hardware Specifications
1. NVIDIA RTX 5070 Ti: Multiple sources (TechReviewer, Puget Systems, VideoCardz)
2. AMD Ryzen 7700x: AMD Official Specifications
3. PCIe 4.0 Bandwidth Analysis: Various technical blogs

### Performance Benchmarks
1. RTX 5070 Ti vs 4070 Ti: Dev Community, Medium articles
2. XGBoost GPU Performance: NVIDIA Technical Blogs
3. Polars Streaming: Polars documentation

### Best Practices
1. XGBoost GPU Optimization: Medium articles, Stack Overflow
2. Optuna Hyperparameter Tuning: Optuna documentation, Medium
3. Memory Management: NVIDIA Technical Blogs, GitHub issues

---

## Conclusion

The RTX 5070 Ti + Ryzen 7700x configuration is **excellent** for XGBoost training:

1. **16GB VRAM** enables training large models without external memory
2. **896 GB/s memory bandwidth** provides fast training
3. **PCIe 4.0** is not a bottleneck for this workload
4. **8 cores/16 threads** sufficient for CPU preprocessing

**Key Optimizations:**
- Use QuantileDMatrix for memory efficiency
- Implement RMM for faster memory allocation
- Optimize Optuna with pruning callbacks
- Feature selection to reduce dimensionality
- Async pipeline to overlap CPU/GPU work

**Expected Performance:**
- **10-30x speedup** vs CPU training
- **1-2 hours** for full universe training with Optuna (vs 16-25 hours on CPU)
- **30-60 seconds** per symbol training (vs 10-15 minutes on CPU)

---

**Last Updated:** January 2025
**Research Duration:** 10+ minutes of comprehensive web research
**Status:** Complete - Ready for implementation
